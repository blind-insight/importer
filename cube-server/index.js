const express = require('express');
const cors = require('cors');
const { BigQuery } = require('@google-cloud/bigquery');
const path = require('path');
const { exec } = require('child_process');
const fs = require('fs');
const os = require('os');

const app = express();
app.use(cors());
app.use(express.json({ limit: '10mb' })); // Increase limit for service account keys

// Mock schema data for testing
const mockBigQuerySchema = {
  fields: [
    {
      name: 'id',
      type: 'INTEGER',
      mode: 'REQUIRED',
      description: 'Unique identifier'
    },
    {
      name: 'name',
      type: 'STRING',
      mode: 'NULLABLE',
      description: 'User name'
    },
    {
      name: 'email',
      type: 'STRING',
      mode: 'REQUIRED',
      description: 'User email address'
    },
    {
      name: 'created_at',
      type: 'TIMESTAMP',
      mode: 'REQUIRED',
      description: 'Record creation timestamp'
    },
    {
      name: 'tags',
      type: 'STRING',
      mode: 'REPEATED',
      description: 'User tags'
    }
  ]
};

// Helper function to parse BigQuery identifiers
function parseDatasetTableId(projectId, datasetId, tableId) {
  // Clean and parse dataset ID
  let cleanDatasetId = datasetId;
  if (datasetId.includes('.')) {
    // If dataset contains dots, take the last part
    const parts = datasetId.split('.');
    cleanDatasetId = parts[parts.length - 1];
  }
  
  // Clean and parse table ID
  let cleanTableId = tableId;
  if (tableId.includes('.')) {
    // If table contains dots, take the last part
    const parts = tableId.split('.');
    cleanTableId = parts[parts.length - 1];
  }
  
  return {
    projectId: projectId.trim(),
    datasetId: cleanDatasetId.trim(),
    tableId: cleanTableId.trim()
  };
}

// Real BigQuery Schema Converter
class BigQuerySchemaConverter {
  constructor(bigQueryInstance) {
    this.bigquery = bigQueryInstance;
  }

  async getTableSchema(datasetId, tableId) {
    try {
      const dataset = this.bigquery.dataset(datasetId);
      const table = dataset.table(tableId);
      const [metadata] = await table.getMetadata();
      return metadata.schema;
    } catch (error) {
      throw new Error(`Failed to fetch table schema: ${error.message}`);
    }
  }

  convertToJsonSchema(bigQuerySchema, tableName) {
    const jsonSchema = {
      type: 'object',
      title: tableName,
      properties: {},
      required: []
    };

    bigQuerySchema.fields.forEach(field => {
      const property = this.convertFieldToJsonSchema(field);
      jsonSchema.properties[field.name] = property;
      
      if (field.mode === 'REQUIRED') {
        jsonSchema.required.push(field.name);
      }
    });

    return jsonSchema;
  }

  convertFieldToJsonSchema(field) {
    const property = {
      description: field.description || ''
    };

    switch (field.type) {
      case 'STRING':
        property.type = 'string';
        break;
      case 'INTEGER':
      case 'INT64':
        property.type = 'integer';
        break;
      case 'FLOAT':
      case 'FLOAT64':
      case 'NUMERIC':
      case 'BIGNUMERIC':
        property.type = 'number';
        break;
      case 'BOOLEAN':
      case 'BOOL':
        property.type = 'boolean';
        break;
      case 'TIMESTAMP':
      case 'DATETIME':
      case 'DATE':
      case 'TIME':
        property.type = 'string';
        property.format = field.type.toLowerCase();
        break;
      case 'RECORD':
        property.type = 'object';
        property.properties = {};
        if (field.fields) {
          field.fields.forEach(subField => {
            property.properties[subField.name] = this.convertFieldToJsonSchema(subField);
          });
        }
        break;
      default:
        property.type = 'string';
    }

    if (field.mode === 'REPEATED') {
      return {
        type: 'array',
        items: property,
        description: field.description || ''
      };
    }

    return property;
  }
}

// Mock Schema Converter (for testing)
class MockSchemaConverter {
  convertToJsonSchema(bigQuerySchema, tableName) {
    const jsonSchema = {
      type: 'object',
      title: tableName,
      properties: {},
      required: []
    };

    bigQuerySchema.fields.forEach(field => {
      const property = this.convertFieldToJsonSchema(field);
      jsonSchema.properties[field.name] = property;
      
      if (field.mode === 'REQUIRED') {
        jsonSchema.required.push(field.name);
      }
    });

    return jsonSchema;
  }

  convertFieldToJsonSchema(field) {
    const property = {
      description: field.description || ''
    };

    switch (field.type) {
      case 'STRING':
        property.type = 'string';
        break;
      case 'INTEGER':
      case 'INT64':
        property.type = 'integer';
        break;
      case 'FLOAT':
      case 'FLOAT64':
      case 'NUMERIC':
      case 'BIGNUMERIC':
        property.type = 'number';
        break;
      case 'BOOLEAN':
      case 'BOOL':
        property.type = 'boolean';
        break;
      case 'TIMESTAMP':
      case 'DATETIME':
      case 'DATE':
      case 'TIME':
        property.type = 'string';
        property.format = field.type.toLowerCase();
        break;
      default:
        property.type = 'string';
    }

    if (field.mode === 'REPEATED') {
      return {
        type: 'array',
        items: property,
        description: field.description || ''
      };
    }

    return property;
  }
}

const mockConverter = new MockSchemaConverter();

// API Routes
app.get('/api/health', (req, res) => {
  res.json({ 
    status: 'OK', 
    message: 'BigQuery Schema Converter API is running (Test Mode)',
    timestamp: new Date().toISOString()
  });
});

app.get('/api/datasets/:datasetId/tables', async (req, res) => {
  const { datasetId } = req.params;
  
  // Mock response
  res.json({ 
    tables: ['users', 'orders', 'products', 'analytics'],
    dataset: datasetId,
    note: 'This is mock data for testing'
  });
});

app.get('/api/datasets/:datasetId/tables/:tableId/schema', async (req, res) => {
  const { datasetId, tableId } = req.params;
  
  try {
    const jsonSchema = mockConverter.convertToJsonSchema(mockBigQuerySchema, tableId);
    
    res.json({
      tableName: tableId,
      dataset: datasetId,
      bigQuerySchema: mockBigQuerySchema,
      jsonSchema,
      note: 'This is mock data for testing. Configure real BigQuery credentials in .env file.'
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// New endpoint for authenticated BigQuery access
app.post('/api/bigquery/schema', async (req, res) => {
  try {
    const { projectId, serviceAccountKey, datasetId, tableId } = req.body;

    // Validate required fields
    if (!projectId || !serviceAccountKey || !datasetId || !tableId) {
      return res.status(400).json({
        error: 'Missing required fields: projectId, serviceAccountKey, datasetId, tableId'
      });
    }

    // Validate service account key format - comprehensive check
    if (typeof serviceAccountKey !== 'object') {
      return res.status(400).json({
        error: 'Service account key must be a JSON object'
      });
    }
    
    const requiredFields = ['type', 'project_id', 'private_key_id', 'private_key', 'client_email', 'client_id'];
    const missingFields = requiredFields.filter(field => !serviceAccountKey[field]);
    
    if (missingFields.length > 0) {
      return res.status(400).json({
        error: `Service account key is missing required fields: ${missingFields.join(', ')}`,
        provided: Object.keys(serviceAccountKey),
        required: requiredFields,
        suggestion: 'Please download a complete service account key JSON file from Google Cloud Console'
      });
    }

    // Parse and clean identifiers
    const { projectId: cleanProjectId, datasetId: cleanDatasetId, tableId: cleanTableId } = parseDatasetTableId(projectId, datasetId, tableId);

    console.log('ðŸ” Input received:');
    console.log(`   Project ID: "${projectId}"`);
    console.log(`   Dataset ID: "${datasetId}"`);
    console.log(`   Table ID: "${tableId}"`);
    console.log('ðŸ”§ Parsed identifiers:');
    console.log(`   Clean Project ID: "${cleanProjectId}"`);
    console.log(`   Clean Dataset ID: "${cleanDatasetId}"`);
    console.log(`   Clean Table ID: "${cleanTableId}"`);

    // Initialize BigQuery with service account credentials
    const bigquery = new BigQuery({
      projectId: cleanProjectId,
      credentials: serviceAccountKey
    });

    const converter = new BigQuerySchemaConverter(bigquery);
    
    console.log(`ðŸ“ Fetching schema for: ${cleanProjectId}.${cleanDatasetId}.${cleanTableId}`);
    const schema = await converter.getTableSchema(cleanDatasetId, cleanTableId);
    const jsonSchema = converter.convertToJsonSchema(schema, cleanTableId);
    
    console.log(`âœ… Successfully fetched schema for ${cleanProjectId}.${cleanDatasetId}.${cleanTableId}`);

    res.json({
      tableName: cleanTableId,
      dataset: cleanDatasetId,
      project: cleanProjectId,
      bigQuerySchema: schema,
      jsonSchema
    });
  } catch (error) {
    console.error('âŒ BigQuery API Error:', error);
    res.status(500).json({ error: error.message });
  }
});

// New endpoint for Blind Insight integration
app.post('/api/blind/create-schema', async (req, res) => {
  const { spawn } = require('child_process');
  
  try {
    const { 
      organization, 
      createNewDataset, 
      datasetName, 
      datasetSlug, 
      schemaName, 
      schemaSlug, 
      fields 
    } = req.body;

    // Validate required fields
    if (!organization || !datasetSlug || !schemaName || !schemaSlug || !fields || fields.length === 0) {
      return res.status(400).json({
        error: 'Missing required fields: organization, datasetSlug, schemaName, schemaSlug, fields'
      });
    }

    console.log('ðŸ” Blind Integration Request:');
    console.log(`   Organization: ${organization}`);
    console.log(`   Create Dataset: ${createNewDataset}`);
    console.log(`   Dataset: ${datasetSlug} (${datasetName || 'existing'})`);
    console.log(`   Schema: ${schemaSlug} (${schemaName})`);
    console.log(`   Fields: ${fields.join(', ')}`);

    // Path to the blind executable
    const blindPath = path.join(process.cwd(), '../../blind/blind');
    
    // Function to execute blind command
    const executeBlindCommand = (args) => {
      return new Promise((resolve, reject) => {
        console.log(`ðŸš€ Executing: ${blindPath} ${args.join(' ')}`);
        
        const blindProcess = spawn(blindPath, args, {
          stdio: ['pipe', 'pipe', 'pipe']
        });

        let stdout = '';
        let stderr = '';

        blindProcess.stdout.on('data', (data) => {
          stdout += data.toString();
        });

        blindProcess.stderr.on('data', (data) => {
          stderr += data.toString();
        });

        blindProcess.on('close', (code) => {
          console.log(`ðŸ“‹ Command output (code ${code}):`);
          console.log(`   stdout: ${stdout}`);
          console.log(`   stderr: ${stderr}`);
          
          if (code === 0) {
            resolve({ stdout, stderr });
          } else {
            reject(new Error(`Blind command failed with code ${code}: ${stderr || stdout}`));
          }
        });

        blindProcess.on('error', (error) => {
          reject(new Error(`Failed to execute blind command: ${error.message}`));
        });
      });
    };

    // Step 0: Check authentication and organization access
    console.log('ðŸ” Checking Blind authentication and organization access...');
    try {
      const orgListResult = await executeBlindCommand(['organization', 'list']);
      console.log('âœ… Authentication successful. Available organizations:');
      console.log(orgListResult.stdout);
      
      // Check if the requested organization exists in the list
      if (!orgListResult.stdout.toLowerCase().includes(`"slug": "${organization}"`)) {
        return res.status(400).json({
          error: `Organization '${organization}' not found or no access. Available organizations: ${orgListResult.stdout}`,
          suggestion: 'Please check your organization slug or ensure you have access to it.'
        });
      }
    } catch (authError) {
      console.log('âŒ Authentication check failed:', authError.message);
      return res.status(401).json({
        error: 'Blind authentication failed. Please ensure you are logged in.',
        details: authError.message,
        suggestion: 'Run: ./blind login'
      });
    }

    // Step 1: Create dataset if requested
    if (createNewDataset && datasetName) {
      try {
        console.log(`ðŸ“¦ Creating dataset: ${datasetName} (${datasetSlug})`);
        const datasetArgs = [
          'dataset', 'create',
          '--organization', organization,
          '--name', datasetName,
          '--slug', datasetSlug
        ];
        
        await executeBlindCommand(datasetArgs);
        console.log(`âœ… Dataset created successfully: ${datasetSlug}`);
      } catch (error) {
        // If dataset already exists, that's OK
        if (error.message.includes('already exists') || error.message.includes('duplicate')) {
          console.log(`â„¹ï¸  Dataset ${datasetSlug} already exists, continuing...`);
        } else if (error.message.includes('400 Bad Request')) {
          console.log(`âš ï¸  Blind API returned 400 Bad Request - this might be a temporary API issue. Continuing without dataset creation...`);
          console.log(`   You may need to create the dataset '${datasetSlug}' manually in Blind Insight`);
        } else {
          throw error;
        }
      }
    }

    // Step 2: Create schema
    console.log(`ðŸ“‹ Creating schema: ${schemaName} (${schemaSlug})`);
    
    // Create temporary schema file
    const tempSchemaFile = path.join(os.tmpdir(), `blind-schema-${Date.now()}.json`);
    
    // Convert fields to Blind JSON Schema format (must be valid JSON Schema)
    const schemaDefinition = {
      type: "object",
      properties: {}
    };
    
    // Parse field definitions (format: "fieldname:type")
    fields.forEach(field => {
      const [fieldName, fieldType] = field.split(':');
      if (fieldName && fieldType) {
        let propertyDef = {};
        
        // Convert to JSON Schema format
        switch (fieldType.toLowerCase()) {
          case 'string':
            propertyDef = {
              type: "string"
            };
            break;
          case 'integer':
            propertyDef = {
              type: "integer",
              minimum: 0,
              maximum: 999999999
            };
            break;
          case 'number':
            propertyDef = {
              type: "number",
              minimum: 0,
              maximum: 999999999,
              precision: 2
            };
            break;
          case 'boolean':
            propertyDef = {
              type: "boolean"
            };
            break;
          default:
            propertyDef = {
              type: "string"
            };
        }
        
        schemaDefinition.properties[fieldName] = propertyDef;
        // schemaDefinition.required.push(fieldName);  // Don't make all fields required by default
      }
    });
    
    // Write JSON Schema definition to temporary file
    fs.writeFileSync(tempSchemaFile, JSON.stringify(schemaDefinition, null, 2));
    console.log(`ðŸ“„ Created temporary JSON Schema file: ${tempSchemaFile}`);
    console.log(`ðŸ“„ JSON Schema content: ${JSON.stringify(schemaDefinition, null, 2)}`);
    
    try {
      const schemaArgs = [
        'schema', 'create',
        '--organization', organization,
        '--dataset', datasetSlug,
        '--name', schemaName,
        '--slug', schemaSlug,
        '--file', tempSchemaFile
      ];

      const schemaResult = await executeBlindCommand(schemaArgs);
      console.log(`âœ… Schema created successfully: ${schemaSlug}`);
      
      // Clean up temporary file
      fs.unlinkSync(tempSchemaFile);
      console.log(`ðŸ—‘ï¸ Cleaned up temporary file: ${tempSchemaFile}`);

      // Step 3: Fetch BigQuery data and insert into Blind schema
      console.log(`ðŸ“Š Fetching data from BigQuery table and inserting into Blind schema...`);
      
      let insertedRecords = 0;
      let dataInsertionError = null;
      
      try {
        // Get the original BigQuery request data from the request body
        const { projectId, serviceAccountKey, datasetId, tableId } = req.body;
        
        console.log(`ðŸ” BigQuery connection info:
   Project ID: ${projectId}
   Dataset ID: ${datasetId}
   Table ID: ${tableId}
   Service Account: ${serviceAccountKey ? 'provided' : 'null'}`);
        
        if (!projectId || !datasetId || !tableId) {
          console.log(`âš ï¸ Skipping data insertion - missing BigQuery connection info`);
          dataInsertionError = 'Missing BigQuery connection info (projectId, datasetId, or tableId)';
        } else {
          // Fetch data from BigQuery
          console.log(`ðŸ“¥ Fetching data from ${projectId}.${datasetId}.${tableId}...`);
          
          let bigQueryData;
          if (serviceAccountKey) {
            // Use real BigQuery connection - run query from user's project
            const userProjectId = serviceAccountKey.project_id; // Get user's project from service account
            const bigquery = new BigQuery({
              projectId: userProjectId, // Use user's project for running jobs
              keyFilename: null,
              credentials: serviceAccountKey
            });
            
            const query = `SELECT * FROM \`${projectId}.${datasetId}.${tableId}\` LIMIT 100`;
            console.log(`ðŸ” Executing query from project ${userProjectId}: ${query}`);
            
            const [rows] = await bigquery.query({ 
              query,
              location: 'US' // Specify location for public datasets
            });
            bigQueryData = rows;
            console.log(`âœ… Fetched ${rows.length} rows from BigQuery`);
          } else {
            // For public data queries without service account, we can't run queries
            // This should not happen since we're checking for projectId/datasetId/tableId
            throw new Error('Service account required for data queries');
          }
          
          if (bigQueryData && bigQueryData.length > 0) {
            // Create temporary file with the data for Blind insertion
            const tempDataFile = path.join(os.tmpdir(), `blind-data-${Date.now()}.json`);
            
            // Convert BigQuery data to format expected by Blind - each record must be wrapped in "data" object
            const blindData = bigQueryData.map(row => {
              const convertedRow = {};
              Object.keys(row).forEach(key => {
                const value = row[key];
                if (value !== null && value !== undefined) {
                  // Find the field definition to determine the correct type
                  const fieldDef = fields.find(field => field.split(':')[0] === key);
                  if (fieldDef) {
                    const fieldType = fieldDef.split(':')[1].toLowerCase();
                    
                    // Convert to the correct type based on the field definition
                    switch (fieldType) {
                      case 'integer':
                        // Handle BigQuery integers - they come as strings or numbers
                        const intValue = typeof value === 'object' && value.value ? value.value : value;
                        const parsedInt = parseInt(String(intValue), 10);
                        convertedRow[key] = isNaN(parsedInt) ? 0 : parsedInt;
                        break;
                      case 'number':
                        // Handle BigQuery numbers - they come as strings or numbers
                        const numValue = typeof value === 'object' && value.value ? value.value : value;
                        const parsedFloat = parseFloat(String(numValue));
                        convertedRow[key] = isNaN(parsedFloat) ? 0.0 : parsedFloat;
                        break;
                      case 'boolean':
                        const boolValue = typeof value === 'object' && value.value ? value.value : value;
                        convertedRow[key] = Boolean(boolValue);
                        break;
                      default:
                        // Handle strings and dates
                        if (typeof value === 'object') {
                          if (value instanceof Date) {
                            convertedRow[key] = value.toISOString();
                          } else if (value.value !== undefined) {
                            convertedRow[key] = String(value.value);
                          } else {
                            convertedRow[key] = JSON.stringify(value);
                          }
                        } else {
                          convertedRow[key] = String(value);
                        }
                    }
                  } else {
                    // If no field definition found, keep as string with proper object handling
                    if (typeof value === 'object') {
                      if (value instanceof Date) {
                        convertedRow[key] = value.toISOString();
                      } else if (value.value !== undefined) {
                        convertedRow[key] = String(value.value);
                      } else {
                        convertedRow[key] = JSON.stringify(value);
                      }
                    } else {
                      convertedRow[key] = String(value);
                    }
                  }
                } else {
                  // Handle null/undefined values - convert to appropriate defaults for each type
                  const fieldDef = fields.find(field => field.split(':')[0] === key);
                  if (fieldDef) {
                    const fieldType = fieldDef.split(':')[1].toLowerCase();
                    switch (fieldType) {
                      case 'integer':
                        convertedRow[key] = 0;
                        break;
                      case 'number':
                        convertedRow[key] = 0.0;
                        break;
                      case 'boolean':
                        convertedRow[key] = false;
                        break;
                      default:
                        convertedRow[key] = '';
                    }
                  } else {
                    convertedRow[key] = '';
                  }
                }
              });
              // Blind Insight expects each record wrapped in a "data" object
              return { data: convertedRow };
            });
            
            fs.writeFileSync(tempDataFile, JSON.stringify(blindData, null, 2));
            console.log(`ðŸ“„ Created temporary data file: ${tempDataFile}`);
            console.log(`ðŸ“„ Sample data (first record): ${JSON.stringify(blindData[0], null, 2)}`);
            
            // Insert data using Blind CLI
            const insertArgs = [
              'record', 'create',
              '--organization', organization,
              '--dataset', datasetSlug,
              '--schema', schemaSlug,
              '--file', tempDataFile
            ];
            
            const insertResult = await executeBlindCommand(insertArgs);
            
            // Parse the result to get record count
            try {
              const resultJson = JSON.parse(insertResult.stdout);
              insertedRecords = resultJson.count || bigQueryData.length;
            } catch (parseError) {
              // If parsing fails, assume all records were inserted
              insertedRecords = bigQueryData.length;
            }
            
            console.log(`âœ… Successfully inserted ${insertedRecords} records into Blind schema`);
            
            // Clean up temporary file
            try {
              fs.unlinkSync(tempDataFile);
              console.log(`ðŸ—‘ï¸ Cleaned up temporary data file: ${tempDataFile}`);
            } catch (cleanupError) {
              console.log(`âš ï¸ Could not clean up temporary data file: ${cleanupError.message}`);
            }
          } else {
            dataInsertionError = 'No data found in BigQuery table';
            console.log(`âš ï¸ No data found in BigQuery table`);
          }
        }
      } catch (error) {
        dataInsertionError = error.message;
        console.log(`âŒ Data insertion error: ${error}`);
      }

      // Return success response with data insertion status
      const successMessage = insertedRecords > 0 
        ? `Blind schema created successfully and ${insertedRecords} records imported` 
        : dataInsertionError 
          ? `Blind schema created successfully but data import failed: ${dataInsertionError}`
          : 'Blind schema created successfully (no data import attempted)';

      return res.json({
        success: true,
        message: successMessage,
        organization,
        dataset: datasetSlug,
        schema: schemaSlug,
        fields,
        jsonSchema: schemaDefinition,
        recordsImported: insertedRecords,
        dataImportError: dataInsertionError
      });
      
    } catch (error) {
      // Clean up temporary file even on error
      try {
        fs.unlinkSync(tempSchemaFile);
        console.log(`ðŸ—‘ï¸ Cleaned up temporary file after error: ${tempSchemaFile}`);
      } catch (cleanupError) {
        console.log(`âš ï¸ Could not clean up temporary file: ${cleanupError.message}`);
      }
      
      // Handle 400 Bad Request errors gracefully
      if (error.message.includes('400 Bad Request')) {
        console.log(`âš ï¸ Blind API returned 400 Bad Request for schema creation. This might be due to:`);
        console.log(`   - Dataset '${datasetSlug}' doesn't exist`);
        console.log(`   - Schema '${schemaSlug}' already exists`);
        console.log(`   - API permissions issue`);
        console.log(`   - Temporary API service issue`);
        
        return res.status(400).json({
          success: false,
          message: `Schema creation failed due to Blind API 400 error. Please check if dataset '${datasetSlug}' exists and schema '${schemaSlug}' is unique.`,
          organization,
          dataset: datasetSlug,
          schema: schemaSlug,
          fields,
          jsonSchema: schemaDefinition,
          error: 'Blind API returned 400 Bad Request',
          suggestion: 'Try creating the dataset manually in Blind Insight first, or use a different schema name.',
          troubleshooting: {
            step1: `Check if dataset '${datasetSlug}' exists in Blind Insight`,
            step2: `Try a different schema name (current: '${schemaSlug}')`,
            step3: 'Verify your Blind Insight permissions',
            step4: 'Check Blind API status'
          }
        });
      }
      
      throw error;
    }

  } catch (error) {
    console.error('âŒ Blind Integration Error:', error);
    return res.status(500).json({ 
      error: error.message,
      details: 'Make sure the Blind Proxy is installed and you are logged in'
    });
  }
});

// New endpoint to check Blind authentication status
app.get('/api/blind/status', async (req, res) => {
  const { spawn } = require('child_process');
  
  try {
    const blindPath = path.join(process.cwd(), '../../blind/blind');
    
    const executeBlindCommand = (args) => {
      return new Promise((resolve, reject) => {
        const blindProcess = spawn(blindPath, args, {
          stdio: ['pipe', 'pipe', 'pipe']
        });

        let stdout = '';
        let stderr = '';

        blindProcess.stdout.on('data', (data) => {
          stdout += data.toString();
        });

        blindProcess.stderr.on('data', (data) => {
          stderr += data.toString();
        });

        blindProcess.on('close', (code) => {
          if (code === 0) {
            resolve({ stdout, stderr });
          } else {
            reject(new Error(`Command failed with code ${code}: ${stderr || stdout}`));
          }
        });

        blindProcess.on('error', (error) => {
          reject(new Error(`Failed to execute command: ${error.message}`));
        });
      });
    };

    // Check organizations
    const orgResult = await executeBlindCommand(['organization', 'list']);
    
    res.json({
      authenticated: true,
      organizations: orgResult.stdout,
      suggestion: 'Use one of the organization slugs shown above'
    });

  } catch (error) {
    res.status(401).json({
      authenticated: false,
      error: error.message,
      suggestion: 'Please run: ./blind login'
    });
  }
});

// Serve the HTML interface from parent directory
app.get('/', (req, res) => {
  res.sendFile(path.join(__dirname, '..', 'index.html'));
});

// Serve the logo file
app.get('/horizontal-light.svg', (req, res) => {
  res.sendFile(path.join(__dirname, '..', 'horizontal-light.svg'));
});

const PORT = process.env.PORT || 3001;
app.listen(PORT, () => {
  console.log(`ðŸš€ Test Schema converter API is running on port ${PORT}`);
  console.log(`ï¿½ï¿½ This is running in test mode with mock data`);
  console.log(`ðŸ”§ Configure BigQuery credentials in .env file for real data`);
});

module.exports = app; 